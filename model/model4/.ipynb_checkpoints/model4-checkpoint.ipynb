{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80dbfcf-0b9d-411d-9a8b-1c5afe58a5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Starting data preprocessing...\n",
      "Resizing target is now 64x64 after brightness enhancement.\n",
      "\n",
      "Processing category 'cracked' (Label 1)...\n",
      "Successfully processed 846 images for 'cracked'.\n",
      "\n",
      "Processing category 'non-cracked' (Label 0)...\n",
      "Successfully processed 873 images for 'non-cracked'.\n",
      "\n",
      "Splitting data into training (80%) and testing (20%) sets...\n",
      "Training images: 1375, Testing images: 344\n",
      "Building CNN model for 64x64 input...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        2432      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64, 64, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 32, 32, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 16, 16, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 8, 8, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 8, 8, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 4, 4, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1048832   \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1441985 (5.50 MB)\n",
      "Trainable params: 1440513 (5.50 MB)\n",
      "Non-trainable params: 1472 (5.75 KB)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting model training with Early Stopping and Learning Rate Reduction...\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 67s 669ms/step - loss: 0.9346 - accuracy: 0.6255 - val_loss: 0.8768 - val_accuracy: 0.5087 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 24s 553ms/step - loss: 0.7863 - accuracy: 0.6589 - val_loss: 1.8581 - val_accuracy: 0.5087 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 21s 497ms/step - loss: 0.7325 - accuracy: 0.6625 - val_loss: 2.5472 - val_accuracy: 0.5087 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.7070 - accuracy: 0.6618\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "43/43 [==============================] - 20s 467ms/step - loss: 0.7070 - accuracy: 0.6618 - val_loss: 2.3596 - val_accuracy: 0.4942 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 23s 536ms/step - loss: 0.6383 - accuracy: 0.7171 - val_loss: 2.4821 - val_accuracy: 0.4884 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.7062Restoring model weights from the end of the best epoch: 1.\n",
      "43/43 [==============================] - 24s 565ms/step - loss: 0.6579 - accuracy: 0.7062 - val_loss: 2.9995 - val_accuracy: 0.4797 - lr: 1.0000e-04\n",
      "Epoch 6: early stopping\n",
      "\n",
      "=======================================================\n",
      "             MODEL EVALUATION ON TEST DATA\n",
      "=======================================================\n",
      "Final Test Loss: 0.8768\n",
      "Final Test Accuracy: 50.87%\n",
      "11/11 [==============================] - 7s 127ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics and Confusion Matrix saved to: model_metrics.csv\n",
      "\n",
      "Five visualization charts saved as PNG files (1-5.png) in the current directory.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Import BatchNormalization and L2 regularization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary components for preprocessing\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageEnhance\n",
    "from typing import Tuple, List\n",
    "\n",
    "# --- Preprocessing Configuration (Copied from image_preprocessor.py) ---\n",
    "DATA_ROOT = r'C:\\Users\\acking\\Desktop\\project\\DeepCrack-An-SDNET2018-Implementation\\raw data\\Walls - Copy'\n",
    "# --- CHANGED: Target size is now 64x64 ---\n",
    "TARGET_SIZE: Tuple[int, int] = (64, 64)\n",
    "BRIGHTNESS_FACTOR: float = 1.5\n",
    "IMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "# --- Model Configuration ---\n",
    "L2_REG_STRENGTH = 1e-4\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "def load_and_preprocess_data(root_dir: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Loads images and applies the requested preprocessing steps:\n",
    "    Brightness enhancement, then resizing, and normalization.\n",
    "    \n",
    "    The order is now: Original Image -> Brightness -> Resizing (64x64) -> Normalization.\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    data: List[np.ndarray] = []\n",
    "    labels: List[int] = []\n",
    "\n",
    "    categories = {'cracked': 1, 'non-cracked': 0}\n",
    "    print(f\"Starting data preprocessing...\")\n",
    "    print(f\"Resizing target is now {TARGET_SIZE[0]}x{TARGET_SIZE[1]} after brightness enhancement.\")\n",
    "\n",
    "    for category, label in categories.items():\n",
    "        folder_path = root_path / category\n",
    "        if not folder_path.is_dir():\n",
    "            print(f\"Warning: Category folder not found: {folder_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing category '{category}' (Label {label})...\")\n",
    "        count = 0\n",
    "\n",
    "        for file_path in folder_path.rglob('*'):\n",
    "            if file_path.suffix.lower() not in IMAGE_EXTENSIONS:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with Image.open(file_path).convert('RGB') as img:\n",
    "                    \n",
    "                    # 1. Increase Brightness (Enhancement)\n",
    "                    # This is applied to the original, potentially large, image\n",
    "                    enhancer = ImageEnhance.Brightness(img)\n",
    "                    img_enhanced = enhancer.enhance(BRIGHTNESS_FACTOR)\n",
    "\n",
    "                    # 2. Resizing (Applied AFTER brightness enhancement)\n",
    "                    img_resized = img_enhanced.resize(TARGET_SIZE)\n",
    "                    \n",
    "                    # 3. Normalize (Convert to array and scale)\n",
    "                    img_array = np.array(img_resized, dtype=np.float32)\n",
    "                    normalized_array = img_array / 255.0\n",
    "\n",
    "                    data.append(normalized_array)\n",
    "                    labels.append(label)\n",
    "                    count += 1\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        print(f\"Successfully processed {count} images for '{category}'.\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"\\nFATAL: No valid images were processed. Cannot build model.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    X = np.array(data)\n",
    "    y = np.array(labels)\n",
    "    return X, y\n",
    "\n",
    "def build_cnn_model(input_shape: Tuple[int, int, int]) -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines an improved Convolutional Neural Network (CNN) architecture.\n",
    "    NOTE: The filter sizes and number of layers are maintained, but since the\n",
    "    input image size is smaller (64x64), the model will run faster and the \n",
    "    number of MaxPooling steps might be adjusted slightly, though the current\n",
    "    structure is generally safe for 64x64.\n",
    "    \"\"\"\n",
    "    print(\"Building CNN model for 64x64 input...\")\n",
    "    # New calculation: 64 -> 32 -> 16 -> 8 -> 4 (Flattened size is smaller now)\n",
    "    model = Sequential([\n",
    "        # Block 1: Input is now 64x64x3\n",
    "        Conv2D(32, (5, 5), activation='relu', input_shape=input_shape, padding='same',\n",
    "               kernel_regularizer=l2(L2_REG_STRENGTH)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)), # Output: 32x32\n",
    "\n",
    "        # Block 2\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "               kernel_regularizer=l2(L2_REG_STRENGTH)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)), # Output: 16x16\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Block 3\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "               kernel_regularizer=l2(L2_REG_STRENGTH)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)), # Output: 8x8\n",
    "        Dropout(0.4),\n",
    "\n",
    "        # Block 4 (Optional for 64x64, but kept for capacity)\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "               kernel_regularizer=l2(L2_REG_STRENGTH)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)), # Output: 4x4\n",
    "        Dropout(0.4),\n",
    "\n",
    "        # Classification Head\n",
    "        Flatten(), # Flattened size is now 4*4*256 = 4096 (was 16*16*256 = 65536)\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_REG_STRENGTH)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid') # Binary classification\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Utility functions (save_error_metrics_csv and generate_plots) remain unchanged\n",
    "def save_error_metrics_csv(y_test: np.ndarray, y_pred_class: np.ndarray, file_path: Path):\n",
    "    \"\"\"\n",
    "    Calculates Confusion Matrix and Classification Report and saves them to a CSV file.\n",
    "    \"\"\"\n",
    "    # Calculate the Confusion Matrix (the 'matrix of errors')\n",
    "    cm = confusion_matrix(y_test, y_pred_class)\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                         index=['Actual Non-Cracked', 'Actual Cracked'],\n",
    "                         columns=['Predicted Non-Cracked', 'Predicted Cracked'])\n",
    "\n",
    "    # Calculate the Classification Report (the 'error metrix')\n",
    "    report = classification_report(y_test, y_pred_class, target_names=['Non-Cracked (0)', 'Cracked (1)'], output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    # Combine data into a single CSV\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"--- Confusion Matrix ---\\n\")\n",
    "        cm_df.to_csv(f)\n",
    "        f.write(\"\\n--- Classification Report (Error Metrics) ---\\n\")\n",
    "        report_df.to_csv(f)\n",
    "\n",
    "    print(f\"\\nMetrics and Confusion Matrix saved to: {file_path.name}\")\n",
    "\n",
    "\n",
    "def generate_plots(history: tf.keras.callbacks.History, y_test: np.ndarray, y_pred_proba: np.ndarray):\n",
    "    \"\"\"\n",
    "    Generates and saves five charts visualizing training history and model performance.\n",
    "    \"\"\"\n",
    "    history_dict = history.history\n",
    "    epochs = range(1, len(history_dict['loss']) + 1)\n",
    "\n",
    "    # --- Chart 1: Accuracy History ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, history_dict['accuracy'], 'bo', label='Training Acc')\n",
    "    plt.plot(epochs, history_dict['val_accuracy'], 'b', label='Validation Acc')\n",
    "    plt.title('1. Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('accuracy_history.png')\n",
    "    plt.close()\n",
    "\n",
    "    # --- Chart 2: Loss History ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, history_dict['loss'], 'ro', label='Training Loss')\n",
    "    plt.plot(epochs, history_dict['val_loss'], 'r', label='Validation Loss')\n",
    "    plt.title('2. Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss_history.png')\n",
    "    plt.close()\n",
    "\n",
    "    # --- Chart 3: Confusion Matrix ---\n",
    "    cm = confusion_matrix(y_test, (y_pred_proba > 0.5).astype(\"int32\"))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('3. Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Non-Cracked', 'Cracked'])\n",
    "    plt.yticks(tick_marks, ['Non-Cracked', 'Cracked'])\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    # --- Chart 4: ROC Curve ---\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('4. Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # --- Chart 5: Precision-Recall Curve ---\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='purple', lw=2, label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('5. Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig('precision_recall_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\nFive visualization charts saved as PNG files (1-5.png) in the current directory.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load data, train, evaluate, and generate reports.\"\"\"\n",
    "\n",
    "    # --- 1. Load and Preprocess Data ---\n",
    "    if not Path(DATA_ROOT).is_dir():\n",
    "        print(f\"ERROR: The main data root directory does not exist at: {DATA_ROOT}\")\n",
    "        print(\"Please verify the path and ensure it contains 'cracked' and 'non-cracked' folders.\")\n",
    "        return\n",
    "\n",
    "    X, y = load_and_preprocess_data(DATA_ROOT)\n",
    "\n",
    "    if X.size == 0:\n",
    "        return\n",
    "\n",
    "    input_shape = X.shape[1:]\n",
    "\n",
    "    # --- 2. Split Data ---\n",
    "    print(\"\\nSplitting data into training (80%) and testing (20%) sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"Training images: {X_train.shape[0]}, Testing images: {X_test.shape[0]}\")\n",
    "\n",
    "    # --- 3. Build Model ---\n",
    "    model = build_cnn_model(input_shape)\n",
    "    model.summary()\n",
    "\n",
    "    # --- 4. Define Callbacks for Training Optimization ---\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=1e-6)\n",
    "    ]\n",
    "\n",
    "    # --- 5. Train Model ---\n",
    "    print(\"\\nStarting model training with Early Stopping and Learning Rate Reduction...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # --- 6. Evaluate Model & Generate Reports ---\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"             MODEL EVALUATION ON TEST DATA\")\n",
    "    print(\"=======================================================\")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    print(f\"Final Test Loss: {loss:.4f}\")\n",
    "    print(f\"Final Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    # Get probability predictions and convert them to class predictions (0 or 1)\n",
    "    y_pred_proba = model.predict(X_test).ravel()\n",
    "    y_pred_class = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Generate and save the CSV of metrics and Confusion Matrix\n",
    "    save_error_metrics_csv(y_test, y_pred_class, Path('model_metrics.csv'))\n",
    "\n",
    "    # Generate and save the five requested charts\n",
    "    generate_plots(history, y_test, y_pred_proba)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
